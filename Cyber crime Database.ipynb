{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nest_asyncio\nimport pandas as pd\nimport csv\nfrom bs4 import BeautifulSoup\n!pip install --upgrade pip\n!pip install playwright\n!pip install chromium\n\nfrom playwright.async_api import async_playwright\n\nnest_asyncio.apply()\n\nasync def load_and_extract_all_links(csv_file=\"cyber_wanted.csv\"):\n    # Load existing links\n    try:\n        df_existing = pd.read_csv(csv_file)\n        existing = set(df_existing[\"Profile URL\"].dropna().tolist())\n    except FileNotFoundError:\n        existing = set()\n\n    print(f\"‚úÖ Currently have {len(existing)} saved links.\")\n\n    async with async_playwright() as pw:\n        browser = await pw.chromium.launch(headless=False)\n        page = await browser.new_page()\n        await page.goto(\"https://www.fbi.gov/wanted/cyber\", timeout=60000)\n        await page.wait_for_timeout(3000)\n\n        # Repeatedly click ‚ÄúShow More‚Äù until it‚Äôs gone or disabled\n        while True:\n            # Scroll into view\n            await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n            await page.wait_for_timeout(1500)\n\n            button = await page.query_selector(\"button.loadMoreButton\")\n            if not button:\n                print(\"üîö No more 'Show More' button.\")\n                break\n\n            disabled_attr = await button.get_attribute(\"disabled\")\n            if disabled_attr is not None:\n                print(\"üîö Button exists but is disabled.\")\n                break\n\n            await button.scroll_into_view_if_needed()\n            await page.evaluate(\"(btn) => btn.click()\", button)\n            print(\"‚û°Ô∏è Clicked 'Show More' to load more entries.\")\n            await page.wait_for_timeout(4000)\n\n        html = await page.content()\n        await browser.close()\n\n    soup = BeautifulSoup(html, \"html.parser\")\n    cards = soup.select(\"li.portal-type-person a\")\n\n    new_links = []\n    for card in cards:\n        href = card.get(\"href\")\n        if href and \"/wanted/cyber/\" in href:\n            full = href if href.startswith(\"http\") else \"https://www.fbi.gov\" + href\n            if full not in existing:\n                new_links.append(full)\n\n    new_links = sorted(set(new_links))\n    print(f\"üîç Found {len(new_links)} new links to add.\")\n\n    if new_links:\n        with open(csv_file, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n            writer = csv.writer(f)\n            for link in new_links:\n                writer.writerow([link])\n        print(f\"‚úÖ Appended {len(new_links)} new links.\")\n    else:\n        print(\"‚ö†Ô∏è No new links to append.\")\n\n    print(f\"üì¶ Final total saved: {len(existing) + len(new_links)} links\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T22:54:31.237867Z","iopub.execute_input":"2025-06-16T22:54:31.238247Z","iopub.status.idle":"2025-06-16T22:54:39.185753Z","shell.execute_reply.started":"2025-06-16T22:54:31.238223Z","shell.execute_reply":"2025-06-16T22:54:39.184546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"await load_and_extract_all_links()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T22:54:39.714558Z","iopub.status.idle":"2025-06-16T22:54:39.714816Z","shell.execute_reply.started":"2025-06-16T22:54:39.714685Z","shell.execute_reply":"2025-06-16T22:54:39.714697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import asyncio\nimport pandas as pd\nfrom playwright.async_api import async_playwright\nfrom bs4 import BeautifulSoup\nimport random\nimport time\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nCSV_FILE = \"cyber_wanted.csv\"\nOUTPUT_FILE = \"cyberwanted_profile.csv\"\n\n# These are the profile fields we care about\nTARGET_FIELDS = [\n    \"Name\", \"Aliases\", \"Date(s) of Birth Used\", \"Place of Birth\", \"Hair\", \"Eyes\",\n    \"Sex\", \"Race\", \"Nationality\", \"Profile URL\"\n]\n\ndef extract_profile_details(html, url):\n    soup = BeautifulSoup(html, 'html.parser')\n    data = {key: \"\" for key in TARGET_FIELDS}\n    data[\"Profile URL\"] = url\n\n    try:\n        data[\"Name\"] = soup.find(\"h1\", class_=\"documentFirstHeading\").text.strip()\n    except: pass\n\n    try:\n        data[\"Aliases\"] = soup.select_one(\"div.wanted-person-aliases p\").text.strip()\n    except: pass\n\n    table = soup.find(\"table\", class_=\"table table-striped wanted-person-description\")\n    if table:\n        rows = table.find_all(\"tr\")\n        for row in rows:\n            try:\n                key = row.find_all(\"td\")[0].text.strip()\n                value = row.find_all(\"td\")[1].text.strip()\n                if key in TARGET_FIELDS:\n                    data[key] = value\n            except: continue\n    return data\n\nasync def scrape_and_save_profiles():\n    df = pd.read_csv(CSV_FILE)\n    urls = df[\"Profile URL\"].dropna().unique().tolist()\n\n    # Load existing results if any\n    try:\n        existing_df = pd.read_csv(OUTPUT_FILE)\n        done_urls = set(existing_df[\"Profile URL\"])\n    except:\n        existing_df = pd.DataFrame(columns=TARGET_FIELDS)\n        done_urls = set()\n\n    results = []\n\n    async with async_playwright() as p:\n        browser = await p.chromium.launch(headless=False)\n\n        for idx, url in enumerate(urls):\n            if url in done_urls:\n                continue\n\n            context = await browser.new_context()\n            page = await context.new_page()\n\n            print(f\"\\n[{idx+1}/{len(urls)}] üîó Opening: {url}\")\n            try:\n                await page.goto(url, timeout=60000)\n                await page.wait_for_selector(\"h1.documentFirstHeading\", timeout=15000)\n                html = await page.content()\n                data = extract_profile_details(html, url)\n\n                print(f\"‚úÖ Extracted: {data['Name']}\")\n                results.append(data)\n\n                # Save to CSV immediately\n                pd.concat([existing_df, pd.DataFrame([data])], ignore_index=True)\\\n                    .drop_duplicates(subset=[\"Profile URL\"])\\\n                    .to_csv(OUTPUT_FILE, index=False)\n                existing_df = pd.read_csv(OUTPUT_FILE)  # Reload updated\n\n            except Exception as e:\n                print(f\"‚ùå Failed to extract {url}: {e}\")\n\n            await context.close()\n\n            # Random wait to mimic human behavior\n            wait_time = random.randint(2, 5)\n            print(f\"‚è≥ Waiting {wait_time} seconds before next...\")\n            time.sleep(wait_time)\n\n        await browser.close()\n\n    print(f\"\\nüìÅ All data saved to: {OUTPUT_FILE}\")\n\n# ‚úÖ Run this in Jupyter or script\nawait scrape_and_save_profiles()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T22:54:39.716362Z","iopub.status.idle":"2025-06-16T22:54:39.716691Z","shell.execute_reply.started":"2025-06-16T22:54:39.716539Z","shell.execute_reply":"2025-06-16T22:54:39.716555Z"}},"outputs":[],"execution_count":null}]}